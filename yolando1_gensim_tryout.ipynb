{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f41fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68476f9e",
   "metadata": {},
   "source": [
    ">**Opening file** Yolando Pino. Cuentos Folkloricos de Chile. Tomo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483fd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/hernanadasme/Documents/yolando_pino/yolando_pino_1.txt\", encoding='utf-8-sig') as yolando_1: \n",
    "    yol_1 = yolando_1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ca7291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880649\n"
     ]
    }
   ],
   "source": [
    "print(len(yol_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95889a65",
   "metadata": {},
   "source": [
    ">**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f19aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 839243\n"
     ]
    }
   ],
   "source": [
    "#filtering text, removing weird characters\n",
    "fil_yol_1 = re.sub('[^\\\\w\\\\s]', '', yol_1)\n",
    "print(type(fil_yol_1), len(fil_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876d9719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 839243\n"
     ]
    }
   ],
   "source": [
    "#lower case the string\n",
    "lower_yol_1 = str.lower(fil_yol_1)\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a62852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 817474\n"
     ]
    }
   ],
   "source": [
    "#replacing newline character, and \n",
    "lower_yol_1 = lower_yol_1.replace('\\n', '')\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b7f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 817387\n"
     ]
    }
   ],
   "source": [
    "#replace specific character\n",
    "lower_yol_1 = lower_yol_1.replace(' s ', '')\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9fb3395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 768564\n"
     ]
    }
   ],
   "source": [
    "#replace specific character\n",
    "lower_yol_1 = lower_yol_1.replace('s ', '')\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f52a9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 768285\n"
     ]
    }
   ],
   "source": [
    "#replace specific character\n",
    "lower_yol_1 = lower_yol_1.replace(' i ', '')\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca80124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 767997\n"
     ]
    }
   ],
   "source": [
    "#replace specific character\n",
    "lower_yol_1 = lower_yol_1.replace(' d ', '')\n",
    "print(type(lower_yol_1), len(lower_yol_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eccbf38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767997 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#create a pattern to remove digits\n",
    "pattern = r\"\\d+\"\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "lower_yol_1 = re.sub(pattern, '', lower_yol_1)\n",
    "print(len(lower_yol_1), type(lower_yol_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848880ea",
   "metadata": {},
   "source": [
    ">**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5165d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hernanadasme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5285ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "143557\n"
     ]
    }
   ],
   "source": [
    "#tokenizing text into words\n",
    "tokenized_yol1 = nltk.word_tokenize(lower_yol_1)\n",
    "print(type(tokenized_yol1))\n",
    "print(len(tokenized_yol1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcabfb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hernanadasme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68cddb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 82551\n"
     ]
    }
   ],
   "source": [
    "#getting rid of no stops \n",
    "#eliminating no stops\n",
    "nostops_yol1 = [ t for t in tokenized_yol1\n",
    "                             if t not in stopwords.words('spanish')]\n",
    "print(type(nostops_yol1), len(nostops_yol1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "013534b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "#changing the format to fit the gensim dictionary, for some reason it needs to be a nested list\n",
    "no_stops_yol1 = [nostops_yol1]\n",
    "print(type(no_stops_yol1), len(no_stops_yol1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ff56b",
   "metadata": {},
   "source": [
    ">**Gensim Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7edd1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary from the articles: dictionary\n",
    "dct_yol1 = Dictionary(no_stops_yol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4ab71d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19809 unique tokens: ['_', '__', '____', '__i', '_epue']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct_yol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "690e534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19809 <class 'gensim.corpora.dictionary.Dictionary'>\n"
     ]
    }
   ],
   "source": [
    "#checking lengths and types\n",
    "print(len(dct_yol1), type(dct_yol1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785b97d",
   "metadata": {},
   "source": [
    ">**Gensim Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb89c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MmCorpus: corpus from the dictionary. list\n",
    "corpus_yol1 = [dct_yol1.doc2bow(doc) for doc in no_stops_yol1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3b4058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_yol1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa0bc4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token to id system\n",
    "#dct_yol1.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "392931e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5132\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"diablo\": diablo_id\n",
    "diablo_id = dct_yol1.token2id.get(\"diablo\")\n",
    "print(diablo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6ce3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#picking one of the list within the list: assuming that each volume is a list within corpus\n",
    "doc_yol1 = corpus_yol1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f912e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the doc for frequency: bow_doc\n",
    "bow_yol1 = sorted(doc_yol1, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28548351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entonce 1012\n",
      "dijo 1009\n",
      "rey 905\n",
      "joven 767\n",
      "jue 579\n",
      "nr 570\n",
      "si 544\n",
      "pa 465\n",
      "dia 443\n",
      "habia 443\n",
      "pp 432\n",
      "onde 410\n",
      "casa 405\n",
      "princesa 391\n",
      "lleg 378\n",
      "va 359\n",
      "tenia 318\n",
      "asi 309\n",
      "ijo 276\n",
      "voy 273\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_yol1[:20]:\n",
    "    print(dct_yol1.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792af58",
   "metadata": {},
   "source": [
    ">**TF-IDF model. Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c724ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85c0062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n",
      "<class 'list'> 19809\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_yol1), len(corpus_yol1))\n",
    "print(type(doc_yol1), len(doc_yol1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d63c4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus_yol1, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4058fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.tfidfmodel.TfidfModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2cd57e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc_yol1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66b7a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70b4a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_tfidf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb3c853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entonce 0.33047194082569675\n",
      "dijo 0.3294922809220633\n",
      "rey 0.2955307375961023\n",
      "joven 0.2504663820289619\n",
      "jue 0.18907436140126327\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dct_yol1.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59b68e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entonce 1012\n",
      "dijo 1009\n",
      "rey 905\n",
      "joven 767\n",
      "jue 579\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_yol1[:5]:\n",
    "    print(dct_yol1.get(word_id), word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
